# -*- coding: utf-8 -*-
"""CosineSim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jH0-68itOu75inMkHqliFBCq-kUBMPRu

- buat biar bisa nambah database
- buat biar bisa kelompokin kategori politik atau olahraga
- buat biar input querry
"""

import nltk

# !pip install Sastrawi

#masukan librari
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# import StemmerFactory class
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

dataset = pd.read_csv('/content/datasetTKI.csv')
docs_key = dataset.iloc[:, 0].values  # d1, d2, d3, etc
docs = dataset.iloc[: , -2].values #buat yang feature / independent
label = dataset.iloc[: , -1].values # politic olahraga etc

print(docs)

print(docs_key) #list
print(docs) #list
print(label)

documents_list = [] # rubah dari numpy array ke list
for i in range(len(docs)):
  documents_list.append(docs[i])

labels_list = [] # rubah dari numpy array ke list
for i in range(len(label)):
  labels_list.append(label[i])

docs_key_list = [] # rubah dari numpy array ke list
for i in range(len(docs_key)):
  docs_key_list.append(docs_key[i])

documents_list, labels_list, docs_key_list

# fungsi untuk nambahin document lagi ke dalam kumpulan document

def tambahDocument():

  while True:
      input_doc = input("Masukkan document baru (ketik 'done' jika sudah): ")
      if input_doc == "done":
          break
      input_label = input("Masukkan kategori untuk melabeli document barusan :")


      documents_list.append(input_doc.lower())
      docs_key_list.append("d{}".format(len(docs_key_list)+1))
      labels_list.append(input_label.upper())
  return documents_list, labels_list

docs_key_list[0]

# Run cell ini jika mau menambahkan document
tambahDocument()

def hapusDocument():
  while True :
    input_cmd = input("masukkan doc yang ingin dihapus, ketik done jika tidak ada : ")

    if input_cmd == "done":
      break
    else :
      index = docs_key_list.index(input_cmd)

      del docs_key_list[index]
      del labels_list[index]
      del documents_list[index]

hapusDocument()

docs_key_list

(documents_list), (labels_list), (docs_key_list)

documents_list

docs_key_list

docs_key_list.index('d3') # parameter adalah dokumen yang mau didelete

def editDocument():
  while True :
    input_cmd = input("masukkan doc yang ingin diedit, ketik done jika tidak ada : ")
    if input_cmd == "done":
      break

    dokumen_baru = input("Masukkan dokumen baru : ")
    label_baru = input("Masukkan label untuk dokumen baru : ")

    index = docs_key_list.index(input_cmd)

    documents_list[index] = dokumen_baru
    labels_list[index] = label_baru

editDocument()

(documents_list), (labels_list), (docs_key_list)

(documents_list), (labels_list), (docs_key_list)

print(type(documents_list))

documents_list

"""metode ini dilakukan dengan menghilangkan stop words sepereti but, not, to, the dan lain lain."""

import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize



from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

def preprocess(sentence):
  # Create an instance of the stop word remover
  factory_stopword = StopWordRemoverFactory()
  factory_stem = StemmerFactory()

  stopword = factory_stopword.create_stop_word_remover()
  stemmer = factory_stem.create_stemmer()

  # Remove the stop words from the sentence
  sentenceStopword = stopword.remove(sentence)
  stopword_stem = stemmer.stem(sentenceStopword)

  return stopword_stem

processed_documents_list = []

for i in range(len(documents_list)):
  tmp = preprocess(documents_list[i])
  processed_documents_list.append(tmp)

processed_documents_list

print(type(documents_list))

documents_list

processed_documents_list

"""## mamasukkan querry yang ingin dicari

"""

query = input("Masukkan querry : ")

"""## Melakukan Cosine similarity"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# preprocessing dan vektorisasi documents
vectorizer = TfidfVectorizer()
vectorized_docs = vectorizer.fit_transform(documents_list)


# preprocessing dan vektorisasi querry
query_vector = vectorizer.transform([query])

# Calculate cosine similarity between the query and each document
similarity_scores = cosine_similarity(query_vector, vectorized_docs)

# Print the similarity scores for each document

for doc_idx, score in enumerate(similarity_scores[0]):
    document = documents_list[doc_idx]
    print(f"Similarity with d{doc_idx+1}: {score:.5f}\t Document: {document} \t Class : {labels_list[doc_idx]}")

query

processed_documents_list

similarity_scores[0]

labels_list

docs_key_list

"""# menyimpulkan kategori dari querry yang dimasukkan"""

len(docs_key_list)

# dictionary of lists
dict = {'document': docs_key_list, 'term yang mewakili dokumen' : processed_documents_list, 'CLASS':labels_list,  'nilai kesamaan': similarity_scores[0]}

df_dokumen = pd.DataFrame(dict)
df_dokumen = df_dokumen[df_dokumen['nilai kesamaan'] != 0]
dokumen_sorted = df_dokumen.sort_values(by=['nilai kesamaan'], ascending=False)
print(dokumen_sorted)

# Use Counter from collections to count unique values in a Python list

from collections import Counter

if len(labels_list) % 2 ==0:
  counter_class = Counter(dokumen_sorted['CLASS'][:-1])
  keys = counter_class.keys()
  num_values = len(keys)

elif len(labels_list) % 2 != 0:
  counter_class = Counter(dokumen_sorted['CLASS'])
  keys = counter_class.keys()
  num_values = len(keys)

keys

num_values

counter_class

"""# menemukan kategori dari querry yang dimasukkan"""

category_querry = max(counter_class, key = counter_class.get)
category_querry

print("'{qry}' termasuk kedalam CLASS : {cat}".format(qry = query, cat = category_querry))

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()

Document1 = "tanding sepakbola persebaya kampanye pemilu 2009 tunda"
Document2 = "	partai golkar demokrat tanding kampanye 2009	"
corpus = [Document1,Document2]

X_train_counts = count_vect.fit_transform(corpus)

pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names_out(),index=['Document 1','Document 2'])



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Example texts
text1 = "tanding sepakbola persebaya kampanye pemilu 2009 tunda"
text2 = "partai golkar demokrat tanding kampanye 2009"
text3 = "tanding pertama persema persebaya malang"

# Create the TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the texts
tfidf_matrix = vectorizer.fit_transform([text1, text2, text3])

# Compute the cosine similarity
cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)

# The cosine similarity matrix
print(cosine_similarities)

